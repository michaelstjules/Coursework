---
title: "CM 764 - Assignment 1"
author: "Michael St. Jules"
date: "January 16, 2017"
output: pdf_document
---


\[ 1. \ a. \ \ \mathbf X = \begin{bmatrix}
1 & 0 \\
1 & 0 \\
0 & 1 \\
0 & 1 \\
0 & 1 \\
0 & 1
\end{bmatrix}, \ \boldsymbol\beta = \begin{bmatrix}
\beta_1 \\
\beta_2
\end{bmatrix}  \ \ \ \ b. \ \ \mathbf X = \begin{bmatrix}
1 & x_1 & 0 \\
1 & x_2 & 0 \\
1 & x_3 & 0 \\
1 & 0 & x_4^2 \\
1 & 0 & x_5^2 \\
1 & 0 & x_6^2
\end{bmatrix}, \ \boldsymbol\beta = \begin{bmatrix}
\beta_1 \\
\beta_2 \\
\beta_3 
\end{bmatrix}\]
\[c. \ \ \mathbf X = \begin{bmatrix}
1 & 0 & x_1 \\
0 & 1 & x_2 \\
1 & 0 & x_3 \\
0 & 1 & x_4 \\
1 & 0 & x_5 \\
0 & 1 & x_6
\end{bmatrix}, \ \boldsymbol\beta = \begin{bmatrix}
\beta_1 \\
\beta_2 \\
\beta_3 
\end{bmatrix} \ \ \ \ d. \ \ \mathbf X = \begin{bmatrix}
1 & x_1 & 0 \\
1 & x_2 & 0 \\
1 & x_3 & 0 \\
1 & 3 & x_4-3 \\
1 & 3 & x_5-3 \\
1 & 3 & x_6-3
\end{bmatrix}, \ \boldsymbol\beta = \begin{bmatrix}
\beta_1 \\
\beta_2 \\
\beta_3 
\end{bmatrix}
\]

(For d., start with ${\beta_1 + \beta_2 x_i, i = 1,2,3}$ and ${\beta_3 + \beta_4 x_i, i = 4,5,6}$, and then use ${\beta_1 + \beta_2 3 = \beta_3 + \beta_4 3}$ to isolate ${\beta_3 = \beta_1 + 3\beta_2 - 3\beta_4}$, and substituting gives ${\beta_1 + 3\beta_2 - 3\beta_4 + \beta_4 x_i = \beta_1 + 3\beta_2 + \beta_4 (x_i-3), i = 4,5,6}$. Then relabel $\beta_4$ as $\beta_3$)







2. a. 
```{r echo=FALSE}
myDirectory <- "/Users/Mike/Desktop/CM 764/A1/"
datafile <- "fakePairs.csv"
completePathname <- paste0(myDirectory, datafile)
fakePairs <- read.csv(completePathname)
```

i = 1

i. The correlation is `r cor(fakePairs$x1, fakePairs$y1)`. The variables are therefore pretty highly correlated.

ii. (The syntax "fit<i>" doesn't seem to work in R, so I use "fiti" instead)

```{r}
fit1 <- lm(fakePairs$y1~fakePairs$x1)
summary.lm(fit1)
```

```{r echo=FALSE} 
s = summary.lm(fit1)
```

iii. $\hat{\boldsymbol\alpha} =$ `r fit1$coefficients[1]`, $\hat{\boldsymbol\beta} =$ `r fit1$coefficients[2]`

iv. The p-value is `r s$coefficients[1,4]` < 2e-16. This is strong evidence against the hypothesis.

v. The p-value is `r s$coefficients[2,4]`  < 2e-16. This is strong evidence against the hypothesis.

vi. $R^2 =`r s$r.squared` \approx 2/3$. So about 2/3 of the variance in $y$ is explained by the model, which is most of it.  







i = 2

i. The correlation is `r cor(fakePairs$x2, fakePairs$y2)`. The variables are therefore pretty highly correlated.

ii.

```{r}
fit2 <- lm(fakePairs$y2~fakePairs$x2)
summary.lm(fit2)
```

```{r echo=FALSE} 
s = summary.lm(fit2)
```

iii. $\hat{\boldsymbol\alpha} =$ `r fit2$coefficients[1]`, $\hat{\boldsymbol\beta} =$ `r fit2$coefficients[2]`

iv. The p-value is `r s$coefficients[1,4]` < 2e-16. This is strong evidence against the hypothesis.

v. The p-value is `r s$coefficients[2,4]`  < 2e-16. This is strong evidence against the hypothesis.

vi. $R^2 =`r s$r.squared` \approx 2/3$. So about 2/3 of the variance in $y$ is explained by the model, which is most of it.  






i = 3

i. The correlation is `r cor(fakePairs$x3, fakePairs$y3)`. The variables are therefore pretty highly correlated.

ii.

```{r}
fit3 <- lm(fakePairs$y3~fakePairs$x3)
summary.lm(fit3)
```

```{r echo=FALSE} 
s = summary.lm(fit3)
```

iii. $\hat{\boldsymbol\alpha} =$ `r fit3$coefficients[1]`, $\hat{\boldsymbol\beta} =$ `r fit3$coefficients[2]`

iv. The p-value is `r s$coefficients[1,4]` < 2e-16. This is strong evidence against the hypothesis.

v. The p-value is `r s$coefficients[2,4]`  < 2e-16. This is strong evidence against the hypothesis.

vi. $R^2 =`r s$r.squared` \approx 2/3$. So about 2/3 of the variance in $y$ is explained by the model, which is most of it.  





i = 3

i. The correlation is `r cor(fakePairs$x4, fakePairs$y4)`. The variables are therefore pretty highly correlated.

ii.

```{r}
fit4 <- lm(fakePairs$y4~fakePairs$x4)
summary.lm(fit4)
```

```{r echo=FALSE} 
s = summary.lm(fit4)
```

iii. $\hat{\boldsymbol\alpha} =$ `r fit4$coefficients[1]`, $\hat{\boldsymbol\beta} =$ `r fit4$coefficients[2]`

iv. The p-value is `r s$coefficients[1,4]` < 2e-16. This is strong evidence against the hypothesis.

v. The p-value is `r s$coefficients[2,4]`  < 2e-16. This is strong evidence against the hypothesis.

vi. $R^2 =`r s$r.squared` \approx 2/3$. So about 2/3 of the variance in $y$ is explained by the model, which is most of it.  



b. The four summaries are all \emph{very} close, at least with respect to the quantities asked about in the previous questions (but they do differ in their min, max, median and 1st and 3rd quantiles), and so they cannot be distinguished on the basis of these alone. As such, one might conclude they come from the same generative model, but the fact that $R^2$ is not higher leaves quite a lot of room for differences. I'm also writing the answer after having seen the plots, and it's pretty obvious that they don't come from the same generative model. 




c. The transparency adjustment doesn't seem to work in RMarkdown, even though it works in R. Here's the code and the plots:

```{r}
par(mfrow=c(2,2))

x1 <- fakePairs$x1
y1 <- fakePairs$y1
plot(x1,y1,col=adjustcolor("black", alpha.f = 0.01))
title("(x1, y1) pairs")
abline(fit1)

x2 <- fakePairs$x2
y2 <- fakePairs$y2
plot(x2,y2,col=adjustcolor("black", alpha.f = 0.01))
title("(x2, y2) pairs")
abline(fit2)

x3 <- fakePairs$x3
y3 <- fakePairs$y3
plot(x3,y3,col=adjustcolor("black", alpha.f = 0.01))
title("(x3, y3) pairs")
abline(fit3)

x4 <- fakePairs$x4
y4 <- fakePairs$y4
plot(x4,y4,col=adjustcolor("black", alpha.f = 0.01))
title("(x4, y4) pairs")
abline(fit4)
```



d. For the first 3 datasets, since the y value is repeated for clustered values of x, it's difficult to attribute any deviation from the line (the residuals) to mean 0 errors, and if there's a true model, it's definitely not a line (or more independent variates are necessary). Piecewise linear functions connecting all of the points actually seem like better guesses, although we can't say what should happen in the gaps.

(x1,y1) looks relatively linear, but the residuals may actually indicate some wavy function (as in my general comment above)

(x2, y2) looks like a quadratic, rather than a line, but because the slope never gets very steep, it's not surprising that a line fits relatively well, with R-squared so high. 

(x3,y3) the second last cluster, the one with the greatest y3 value, seems like an outlier, but because it's a cluster and not a single point, it seems unlikely this can be attributed to random noise. Two possibilities come to mind: it could be a "systematic error" or the y3 values could really be that way. Ignoring that cluster, however, a straight line model seems like it would be a near perfect fit. I would have expected R-squared to be larger than ~2/3. 

(x4,y4) looks like a line could be the "least wrong" for it if we assume that intermediate x4 values are possible, but only because the only x4 values are 8 and 19, so a line is pretty useless with such a large gap. Basically all of the variance in y4 happens at x4=8, and it's not hard to fit a line to two points. 




```{r echo=FALSE}
myDirectory <- "/Users/Mike/Desktop/CM 764/A1/"
datafile <- "Advertising.csv"
completePathname <- paste0(myDirectory, datafile)
Advertising <- read.csv(completePathname)
```

3. a. 

```{r}
plot(Advertising$TV, Advertising$Sales, main="Sales vs TV", xlab = "TV", ylab = "Sales")
# i.
fit1 <- lm(Advertising$Sales~Advertising$TV)
# ii.
abline(fit1)
# iii.
Xorder <- order(Advertising$TV)
pred <- suppressWarnings(predict(fit1, interval = c("prediction"), level = 0.95))
lines(Advertising$TV[Xorder],pred[Xorder,2])
lines(Advertising$TV[Xorder],pred[Xorder,3])
```


b. i. Since $k(x) > 0$, ${Y_i = \mu(x_i) + R_i \iff Y_i/k(x_i) = \mu(x_i)/k(x_i) + R_i/k(x_i)}$, and ${R_i/k(x_i) \sim N(0,\sigma^2) \iff R_i \sim N(0,\sigma^2 k(x_i)^2)}$, so take 

\[R_i^* = R_i/k(x_i), \ Y_i^* = Y_i/k(x_i), \ \mu^*(x_i) = \mu(x_i)/k(x_i)\]

ii. To show that the two models are equivalent, we want to find $w_i \neq 0$ such that 
\[(r_i^*)^2 = w_i(r_i)^2, \ (y_i^*-mu^*(x_i))^2 = w_i(y_i-\mu(x_i))^2. \]
From i., $w_i = \tfrac{1}{k(x_i)^2} > 0$ works. Since $w_i \neq 0$, the transformations from one model to the other are invertible. 

The weights are \emph{inversely proportional} to the variances ${Var(R_i) = \sigma^2 k(x_i)^2}$, as ${w_i = \tfrac{1}{k(x_i)^2} = \dfrac{\sigma^2}{Var{R_i}}}$. So, as $Var(R_i)$ increases, $w_i$ decreases, and the points with the greatest weight $w_i$ will be those with the least residual variability.



iii. Here, I don't interpret the weight $w_i$ and the influence of $(x_i,y_i)$ to be the same. Looking at the plot (without weights), the number of points around each $x$ value seems to be relatively consistent, so it is not from this that some points have more or less influence. However, the points furthest from the line, which occur mostly towards to the right, had the most influence, since the values of $r_i$ were bound to be high for these. 
#because there's more spread
# Cook's distance! Leverage.

For $w_i = x_i$, the points with the largest $x_i$ receive the greatest weight, and those with the smallest $x_i > 0$ receive the smallest weight. So, this gives the rightmost points, with the largest $x_i$, \emph{even more influence}. So, the rightmost points have the most influence. 

For $w_i = 1/x_i$, the points with the largest $x_i$ receive the smallest weight, and those with the smallest $x_i > 0$ receive the greatest weight. This is opposite to the unweighted influence, and since the spread of the variance of the $y_i$ seems to scale roughly linearly, this should help to even out the influence of all the points. The prediction intervals in part iv. seem to agree with this.


iv.

```{r}
#w_i = x_i
plot(Advertising$TV, Advertising$Sales, main="Sales vs TV, with w_i = x_i", xlab = "TV", ylab = "Sales")
fit2 <- lm(Advertising$Sales~Advertising$TV, weights=Advertising$TV)
abline(fit2)
pred2 <- suppressWarnings(predict(fit2, interval = c("prediction"), level = 0.95, weights=Advertising$TV))
lines(Advertising$TV[Xorder], pred2[Xorder,2], lty=2)
lines(Advertising$TV[Xorder], pred2[Xorder,3], lty=2)

# w_i = 1/x_i
plot(Advertising$TV, Advertising$Sales, main="Sales vs TV, with w_i = 1/x_i", xlab = "TV", ylab = "Sales")
fit3 <- lm(Advertising$Sales~Advertising$TV, weights=1./Advertising$TV)
abline(fit3)
pred3 <- suppressWarnings(predict(fit3, interval = c("prediction"), level = 0.95, weights=1./Advertising$TV))
lines(Advertising$TV[Xorder], pred3[Xorder,2], lty=2)
lines(Advertising$TV[Xorder], pred3[Xorder,3], lty=2)
```

In the first plot, with $w_i = x_i$, the lengths of prediction intervals quickly scale to infinity near $x=0$, since the nearby points are given almost no weight and have almost no influence on the fit; they are effectively ignored (if given weight 0, they would be literally ignored, and so we'd expect the prediction intervals to be infinite, because the fit would have nothing to say about them). The line and the prediction intervals fit the leftmost data \emph{extremely} poorly, as expected since they're given little weight, but the rightmost data relatively well, also as expected, since they're given much more weight.

In the second plot, in the first plot, with $w_i = 1/x_i$, the lengths of prediction intervals grow from left to right, instead. Since the leftmost points receive extremely high weight, their prediction intervals are much narrower (converging to a point at $x=0$). The line fits the leftmost data better, but is too high on the right. Because the data seem to be arranged more like $y \approx \beta \sqrt{x}$, however, the first two points are given so much weight that the line is shifted below almost all of the other data until about $x=65$. While the prediction intervals capture almost all of the data, they are also much larger than they need to be (had $\mu(x) = \beta \sqrt{x}$ been used instead or had the first two points been removed). 






c.

```{r}

# i.
fit1 <- lm(Advertising$Sales~Advertising$TV)
ab_res1 <- abs(fit1$residuals)

# ii.
plot(fit1$fitted.values,ab_res1, main="ab_res1 vs fit1$fitted.values")

# iii.
sfit <- lm(ab_res1~fit1$fitted.values)
abline(sfit)

# iv.
fit2 <- lm(Advertising$Sales~Advertising$TV, weights=1/sfit$fitted.values^2)
plot(Advertising$TV, Advertising$Sales, main="Sales vs TV, with fit2", xlab = "TV", ylab = "Sales")
abline(fit2)
pred2 <- suppressWarnings(predict(fit2, interval = c("prediction"), level = 0.95, weights=1/sfit$fitted.values^2))
lines(Advertising$TV[Xorder], pred2[Xorder,2], lty=2)
lines(Advertising$TV[Xorder], pred2[Xorder,3], lty=2)
```

d. fit2 from c. fits the data the best of all those seen so far (a., b. and c.), as the prediction intervals are relatively tight to the data and only miss two or 3 points (including the problematic first two points). All of the other fits (from a. and b.), on the other hand, had prediction intervals that were too large for much of the data. The line from fit2 also fits the leftmost points much better than the first unweighted fit (a.), while still fitting the rightmost points almost just as well (it actually doesn't look any worse to me).

However, the fact that the weights are estimated from the data is not accounted for in the construction of the prediction intervals, so I'm not sure if this is theoretically sound: the assumptions about the distributions of the residuals used to estimate the prediction intervals may no longer hold. 

(Also interesting is the fact that the weights come from the residuals of a previous fit, so we can repeat this procedure, using the newest residuals to calculate weights again, and then fit again. Would repeating this multiple times improve the fit? Would this procedure converge to the solution of a suitably defined minimization problem, perhaps the minimization of the weighted sum of residuals, where the weights are subject to some constraint (e.g. have a fixed norm)? I did exactly this, and it seems to have converged in just one extra step, widening the prediction intervals slightly towards the right and just barely increasing the slope of the line.)

# It's a fixed point?
# minimized distance between w_i/r_i^2 and 1 for each i?












4. 

a. 

Spearman's rho is used to test for a monotonic relationship between the $x_i$ and $y_i$. It is Pearson's correlation coefficient applied to the ranks of the $x_i$ and $y_i$, $\mathrm{rg} (x_i)$ and $\mathrm{rg} (y_i)$, i.e.

\begin{equation} r_s = \dfrac{\sum_{i=1}^N (\mathrm{rg} (x_i) - \overline{\mathrm{rg} (x)})(\mathrm{rg} (y_i) - \overline{\mathrm{rg} (y)})}{\sqrt{\sum_{i=1}^N(\mathrm{rg} (x_i) - \overline{\mathrm{rg} (x)})^2 \sum_{i=1}^N(\mathrm{rg} (y_i) - \overline{\mathrm{rg} (y)})^2}} \end{equation}

where $\mathrm{rg} (x_i) \in \{1, \dots, N\}$ is strictly increasing in $x_i$, $\overline{\mathrm{rg} (x)} = \tfrac{1}{N} \sum_{i=1}^N \mathrm{rg} (x_i)$, and similarly for $\mathrm{rg} (y_i)$ and $\overline{\mathrm{rg} (y)}$

When there are no ties, 
\begin{equation} r_s = 1 - \dfrac{6 \sum_{i=1}^N (\mathrm{rg} (x_i)-\mathrm{rg} (y_i))^2 }{N(N^2-1)}
\end{equation}

In R, the function cor.test with the option method="spearman" computes the Spearman correlation. 

One concern is when there are ties, so that the second formula cannot be applied, and the first should be used instead. There are corrected formulas for ties (and R corrects for ties by default). With respect to just assigning the ranks, the average rank is used (i.e. $m$ equal $x_i$ would receive $m$ consecutive ranks, but instead are all assigned the average of the $m$; this would be the same as breaking ties uniformly at random and then averaging over the resulting assignments). See <ftp://biostat.wisc.edu/pub/chappell/800/hw/spearman.pdf> for a summary.

b. 

Spearman's rho is more robust, since it deals only with ranks, and so the magnitudes of outliers don't figure into its calculation beyond the rank they receive, i.e. $x = 1.5, 3.5, 6, 7, 10000$ have ranks $1,2,3,4,5$. Furthermore, Spearman's rho tests for monotonic relationships, which are more general than linear relationships, and is invariant under transformations which preserve the rank, including strictly increasing transformations. 

c. 

Both correlations are between -1 and 1 (by the Cauchy-Schwartz inequality). Pearson's correlation captures linear relationships berween $x$ and $y$. This can partially be explained by the fact that it is invariant under affine transformations (i.e. $z \mapsto az + b$) of $x$ and/or $y$ (with nonzero slope), which are closed under composition. If $y_i = a x_i + b$, then Pearson's correlation is just the sign of $a$, and so has absolute value 1. (And this is actually if and only if.)

Spearman's, on the other hand, captures monotonic relationships: Spearman's is invariant under strictly increasing transformations (which are closed under composition) to $x$ and/or $y$ (and monotonic means increasing only or decreasing only, but not necessarily strictly) because these preserve the ranks. If $y_i = f(x_i)$ where $f$ is some strictly increasing function (with no ties), then $\mathrm{rg}(y_i) = \mathrm{rg}(x_i)$ for each $i$, so $\mathrm{rg}(x_i) - \mathrm{rg}(y_i) = 0$ for each $i$, and Spearman's rho is just 1. If $f$ is strictly decreasing (with no ties), then Spearman's rho is -1. (And these are actually if and only if)